{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_Plant_Patho_2021_InceptionResNetV2_DataSet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vuj4axkyZ4HM",
        "outputId": "550bb650-f9b6-4550-8cb0-332326c69a90"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-ttKQtNaTdH",
        "outputId": "8f653222-0058-4c2f-c59a-63db59abf6ac"
      },
      "source": [
        "cd Plant"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Plant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lLnWauVaKMH"
      },
      "source": [
        "!mkdir Plant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFCgF1iKaZ2t",
        "outputId": "3052d877-c68e-4340-ebef-257601ad6433"
      },
      "source": [
        "cd Plant/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Plant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHP-8RCtcRUl"
      },
      "source": [
        "!mkdir test_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNeLUw64hki-"
      },
      "source": [
        "!mkdir train_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hRkIV51ykmXd",
        "outputId": "069fa0a8-8c1f-407f-f46b-6c3b08125b40"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/Plant'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3Z8IkpFki6V",
        "outputId": "26eb5de3-177f-4907-ae74-e4db0a75ed2f"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_submission.csv  train_comp.csv  train_images_comp.rar\n",
            "\u001b[0m\u001b[01;34mtest_images\u001b[0m/           \u001b[01;34mtrain_images\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmVsO9HDcUVJ"
      },
      "source": [
        "# Decompression du fichier .rar des 18632 (la totalité) des images labellisées et compressées\n",
        "!unrar x train_images_comp.rar train_images "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaTgrwWqhtEA",
        "outputId": "927e8648-6462-4e75-9a97-67bdc1d6b62b"
      },
      "source": [
        "ls -1 /content/drive/MyDrive/Colab\\ Notebooks/Plant/train_images/ | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-9jtXt9Biwo",
        "outputId": "c8d79b30-93d6-4306-a297-2a9268016171"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kc7mp4OtWZ2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "from time import time\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MibxNTgF8SA",
        "outputId": "ed1631c2-a3aa-4344-c3c5-74ce2221fbc9"
      },
      "source": [
        "# load dataset\n",
        "\n",
        "# sur colab\n",
        "# TRAIN_IMAGES_PATH = \"F:/Projet_computer_vision/plant-pathology-2021-fgvc8/train_images/\"\n",
        "\n",
        "# # en local\n",
        "# TRAIN_IMAGES_PATH = \"F:/Projet_computer_vision/plant-pathology-2021-fgvc8/compressed/train_images/\"\n",
        "# TEST_IMAGES_PATH = \"F:/Projet_computer_vision/plant-pathology-2021-fgvc8/test_images/\"\n",
        "# sur colab\n",
        "TRAIN_IMAGES_PATH = \"drive/MyDrive/Colab Notebooks/Plant/train_images/\"\n",
        "TEST_IMAGES_PATH = \"drive/MyDrive/Colab Notebooks/Plant/test_images/\"\n",
        "\n",
        "# en local\n",
        "# TRAIN_PATH = \"F:/Projet_computer_vision/plant-pathology-2021-fgvc8/compressed/train_comp.csv\"\n",
        "# SUB_PATH = \"F:/Projet_computer_vision/plant-pathology-2021-fgvc8/sample_submission.csv\"\n",
        "\n",
        "# sur colab\n",
        "TRAIN_PATH = \"drive/MyDrive/Colab Notebooks/Plant/train_comp.csv\"\n",
        "SUB_PATH = \"drive/MyDrive/Colab Notebooks/Plant/sample_submission.csv\"\n",
        "\n",
        "train_data = pd.read_csv(TRAIN_PATH)\n",
        "print(\"train_data= \" , train_data[:5],\"\\n\")\n",
        "test_data = pd.read_csv(SUB_PATH)\n",
        "print(\"test_data= \", test_data,\"\\n\")\n",
        "# sub = pd.read_csv(SUB_PATH)\n",
        "\n",
        "# bon 3\n",
        "classes=set()\n",
        "for v in train_data[\"labels\"].values:\n",
        "    classes.update(set(v.split()))\n",
        "print(\"classes= \", classes,\"\\n\")\n",
        "\n",
        "#On peut associer les images des feuilles à 5 classes possibles:\n",
        "#'complex', 'frog_eye_leaf_spot', 'powdery_mildew', 'rust', 'scab'\n",
        "# healthy , quand aucune de ces classes n'est là.\n",
        "\n",
        "train_data['scab']=train_data['labels'].apply(lambda x: 1 if 'scab' in x else 0) \n",
        "train_data['rust']=train_data['labels'].apply(lambda x: 1 if 'rust' in x else 0) \n",
        "train_data['powdery_mildew']=train_data['labels'].apply(lambda x: 1 if 'powdery_mildew' in x else 0) \n",
        "train_data['frog_eye_leaf_spot']=train_data['labels'].apply(lambda x: 1 if 'frog_eye_leaf_spot' in x else 0) \n",
        "train_data['complex']=train_data['labels'].apply(lambda x: 1 if 'complex' in x else 0) \n",
        "# train_data\n",
        "print(\"train_data ave 5 colonnes de labels = \")\n",
        "print(train_data[:3],\"\\n\")\n",
        "\n",
        "\n",
        "# sur colab\n",
        "def add_path_compressed_train(file):\n",
        "    return TRAIN_IMAGES_PATH  + file\n",
        "\n",
        "def add_path_test(file):\n",
        "    return TEST_IMAGES_PATH + file\n",
        "\n",
        "train_files= train_data.image.map(add_path_compressed_train).values\n",
        "print(\"train_files= \\n\", train_files[:3],\"\\n\")\n",
        "\n",
        "test_files= test_data.image.map(add_path_test).values\n",
        "print(\"test_files= \\n\", test_files,\"\\n\")\n",
        "\n",
        "labels = np.float32(train_data.loc[:, 'scab':'complex'].values)\n",
        "print(\"labels= \\n \", labels[:3],\"\\n\")\n",
        "\n",
        "print(\"shapes= \", train_files.shape, labels.shape,\"\\n\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data=                               image                           labels\n",
            "0  Compressed_800113bb65efe69e.jpg                          healthy\n",
            "1  Compressed_8002cb321f8bfcdf.jpg  scab frog_eye_leaf_spot complex\n",
            "2  Compressed_80070f7fb5e2ccaa.jpg                             scab\n",
            "3  Compressed_80077517781fb94f.jpg                             scab\n",
            "4  Compressed_800cbf0ff87721f8.jpg                          complex \n",
            "\n",
            "test_data=                    image   labels\n",
            "0  85f8cb619c66b863.jpg  healthy\n",
            "1  ad8770db05586b59.jpg  healthy\n",
            "2  c7b03e718489f3ca.jpg  healthy \n",
            "\n",
            "classes=  {'rust', 'frog_eye_leaf_spot', 'scab', 'healthy', 'complex', 'powdery_mildew'} \n",
            "\n",
            "train_data ave 5 colonnes de labels = \n",
            "                             image  ... complex\n",
            "0  Compressed_800113bb65efe69e.jpg  ...       0\n",
            "1  Compressed_8002cb321f8bfcdf.jpg  ...       1\n",
            "2  Compressed_80070f7fb5e2ccaa.jpg  ...       0\n",
            "\n",
            "[3 rows x 7 columns] \n",
            "\n",
            "train_files= \n",
            " ['drive/MyDrive/Colab Notebooks/Plant/train_images/Compressed_800113bb65efe69e.jpg'\n",
            " 'drive/MyDrive/Colab Notebooks/Plant/train_images/Compressed_8002cb321f8bfcdf.jpg'\n",
            " 'drive/MyDrive/Colab Notebooks/Plant/train_images/Compressed_80070f7fb5e2ccaa.jpg'] \n",
            "\n",
            "test_files= \n",
            " ['drive/MyDrive/Colab Notebooks/Plant/test_images/85f8cb619c66b863.jpg'\n",
            " 'drive/MyDrive/Colab Notebooks/Plant/test_images/ad8770db05586b59.jpg'\n",
            " 'drive/MyDrive/Colab Notebooks/Plant/test_images/c7b03e718489f3ca.jpg'] \n",
            "\n",
            "labels= \n",
            "  [[0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 1.]\n",
            " [1. 0. 0. 0. 0.]] \n",
            "\n",
            "shapes=  (18632,) (18632, 5) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_f8xx68Bu2f"
      },
      "source": [
        "def decode_image(filename,label=None,  image_size=(512, 512)):\n",
        "    bits = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(bits, channels=3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = tf.image.resize(image, image_size)\n",
        "    if label is None:\n",
        "        return image\n",
        "    else:\n",
        "        return image, label\n",
        "\n",
        "def data_augment(image, label=None):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    \n",
        "    if label is None:\n",
        "        return image\n",
        "    else:\n",
        "        return image, label\n",
        "    \n",
        "def make_dataset(train_images, train_labels, valid_images, valid_labels, test_files, image_size,batch_size):\n",
        "    \n",
        "    train_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices((train_images, train_labels))   \n",
        "        #.from_tensor_slices((train_images[:10], train_labels[:10]))   \n",
        "        .map(tf.autograph.experimental.do_not_convert(lambda x,y: decode_image(x,y, image_size)), num_parallel_calls=AUTO )           \n",
        "        #.map(decode_image, num_parallel_calls=AUTO)\n",
        "        .map(data_augment, num_parallel_calls=AUTO)\n",
        "        .repeat()\n",
        "        .shuffle(512)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "    valid_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices((valid_images, valid_labels))\n",
        "        # .from_tensor_slices((valid_images[:2], valid_labels[:2]))\n",
        "        # .map(decode_image, num_parallel_calls=AUTO)\n",
        "        .map(tf.autograph.experimental.do_not_convert(lambda x,y: decode_image(x,y, image_size)), num_parallel_calls=AUTO )           \n",
        "        .batch(batch_size)\n",
        "        .cache()\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "    test_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices(test_files)\n",
        "        .map(tf.autograph.experimental.do_not_convert(lambda x: decode_image(x,None, image_size)), num_parallel_calls=AUTO )           \n",
        "        .batch(batch_size)\n",
        "    )\n",
        "    return train_dataset,valid_dataset,test_dataset\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "def get_model(image_shape):\n",
        "    # InRes3\n",
        "    #Adding the final layers to the above base models where the actual classification is done in the dense layers\n",
        "    InResNetV2= InceptionResNetV2(include_top=False, weights=\"imagenet\", \\\n",
        "        input_tensor=None, input_shape=image_shape, pooling='avg') \n",
        "    model_InResNet2 = Sequential()\n",
        "    model_InResNet2.add(InResNetV2)\n",
        "    model_InResNet2.add(Dense(5, activation=('sigmoid')))\n",
        "\n",
        "    model_InResNet2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    model_InResNet2.summary()\n",
        "    return model_InResNet2\n",
        "\n",
        "\n",
        "def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n",
        "               lr_min=0.00001, lr_rampup_epochs=5, \n",
        "               lr_sustain_epochs=0, lr_exp_decay=.8):\n",
        "    # lr_max = lr_max * strategy.num_replicas_in_sync\n",
        "\n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_rampup_epochs:\n",
        "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
        "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
        "            lr = lr_max\n",
        "        else:\n",
        "            lr = (lr_max - lr_min) *\\\n",
        "                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n",
        "                                - lr_sustain_epochs) + lr_min\n",
        "        return lr\n",
        "    return lrfn   \n",
        "\n",
        "\n",
        "def train(model, train_dataset, valid_dataset,epochs, len_train,  batch_size, filenum):\n",
        "         \n",
        "    \n",
        "    # learning rate\n",
        "    lrfn = build_lrfn()\n",
        "    STEPS_PER_EPOCH = len_train // batch_size\n",
        "    # STEPS_PER_EPOCH = 10\n",
        "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n",
        "    \n",
        "    # Early Stopping\n",
        "    early_stop= EarlyStopping(monitor='val_accuracy',patience=5, mode='max' )\n",
        "    \n",
        "    # save the weights given the best val_loss\n",
        "    filepath='drive/MyDrive/Colab Notebooks/Plant/IncRestV2-best-dataset-062021-'+str(filenum)+'.hdf5'\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "    # callbacks_list = [lr_schedule,checkpoint ]\n",
        "    callbacks_list = [lr_schedule,early_stop, checkpoint ]\n",
        "    \n",
        "    Network=model.fit(train_dataset, steps_per_epoch=STEPS_PER_EPOCH, verbose=1, \\\n",
        "                            validation_data=valid_dataset,callbacks=callbacks_list,epochs=epochs)\n",
        "    \n",
        "    return Network\n",
        "    \n",
        "    \n",
        "def validate(model, valid_dataset,len_valid,n):    # exactement : accuracy_score(valid_true_labels, np.round(valid_pred)) de sklearn\n",
        "   \n",
        "    # batch_size= 16\n",
        "    # transforme le dataset des labels de validation en numpy ndarray \n",
        "    def pre_validation(valid_dataset):\n",
        "\n",
        "        valid_true_labels=np.zeros((len_valid,5))\n",
        "\n",
        "        for i, x in enumerate(valid_dataset):           \n",
        "                for  j in range(16):\n",
        "                    if j + 16 * i  < len_valid:                       \n",
        "                        valid_true_labels[j+ 16 * i]= x[1][j]            \n",
        "                     # print(i)\n",
        "                    else:\n",
        "                        break;               \n",
        "\n",
        "        return valid_true_labels\n",
        "    \n",
        "    valid_true_labels=  pre_validation(valid_dataset)\n",
        "    \n",
        "    model = load_model(\"drive/MyDrive/Colab Notebooks/Plant/IncRestV2-best-dataset-062021-\"+str(n)+\".hdf5\")\n",
        "    valid_pred = model.predict(valid_dataset)\n",
        "    \n",
        "    # c'est exactement : accuracy_score(valid_true_labels, np.round(valid_pred)) (from sklearn.metrics import accuracy_score)\n",
        "    accuracy= np.mean(np.all(valid_true_labels == np.round(valid_pred), axis=1))\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "# une autre manière pour calculer la validation accuracy qui donne le même résultat.\n",
        "def validate1(model, valid_dataset,len_valid,n): \n",
        "   \n",
        "    def pre_validation(valid_dataset):\n",
        "\n",
        "        valid_true_labels=np.zeros((len_valid,5))\n",
        "\n",
        "        for i, x in enumerate(valid_dataset):           \n",
        "                for  j in range(16):\n",
        "                    if j + 16 * i  < len_valid:\n",
        "                        \n",
        "                        valid_true_labels[j+ 16 * i]= x[1][j]            \n",
        "                    \n",
        "                    else:\n",
        "                        break;               \n",
        "\n",
        "        return valid_true_labels\n",
        "    \n",
        "    valid_true_labels=  pre_validation(valid_dataset)\n",
        "    \n",
        "    \n",
        "    valid_pred = model.predict(valid_dataset)\n",
        "    accuracy_score(valid_true_labels, np.round(valid_pred))\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "# fonction qui trace les 2 courbes loss et validation loss ainsi que l'accuracy et la validation accuracy\n",
        "def loss_accuracy_curves(Network) :\n",
        "    plt.subplot(121)\n",
        "    plt.plot(Network.history['loss'],label='Train')\n",
        "    plt.plot(Network.history['val_loss'],label='Validation')\n",
        "    plt.ylabel('Loss',fontsize=20)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['Training', 'Validation'], loc='best')  # upper\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.plot(Network.history['accuracy'],label='Train')\n",
        "    plt.plot(Network.history['val_accuracy'],label='Validation')\n",
        "    plt.ylabel('Accuracy',fontsize=20)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['Training', 'Validation'], loc='best')  # upper\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJWKHBR1Cblu",
        "outputId": "075d8275-1c10-447e-d835-a58f911412e4"
      },
      "source": [
        "image_size=(100, 100)\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "batch_size =16\n",
        "\n",
        "X = train_files   # fichiers des images des feuilles (18 632)\n",
        "y = labels     # dataframes des 5 colonnes (1 ou 0) des pathologies\n",
        "cv = RepeatedKFold(n_splits=3, n_repeats=1, random_state=1)\n",
        "\n",
        "debut= time()\n",
        "\n",
        "def evaluate_model_plant_patho(X,y):\n",
        "    n=1\n",
        "    list_val_accuracy=[]\n",
        "    for train_ix, test_ix in cv.split(X):\n",
        "        \n",
        "        print(\"Split des données entre train set et validation set en utlisant la 3-fold cross validation \")\n",
        "        train_images, valid_images = X[train_ix], X[test_ix]\n",
        "        train_labels, valid_labels = y[train_ix], y[test_ix]\n",
        "        \n",
        "        print(train_images.shape,  train_labels.shape)  # 12 421 ou 12 422\n",
        "        print(valid_images.shape, valid_labels.shape) # len_valid= len(train_data) // 3  (6 210)  \n",
        "                                                    # ou len_valid= len(train_data) // 3 + 1  (6 211)\n",
        "\n",
        "        \n",
        "        len_train = train_images.shape[0] \n",
        "        len_valid=valid_images.shape[0]\n",
        "        \n",
        "        print(len_train, len_valid)\n",
        "        \n",
        "        print(\"\\nCréation des datasets train, validation et test\")\n",
        "        train_dataset,valid_dataset,test_dataset= make_dataset(train_images, train_labels, valid_images, valid_labels, test_files,\\\n",
        "                                                               image_size, batch_size)\n",
        "        \n",
        "        \n",
        "        \n",
        "        print(\"\\nCréation du modèle à partir d'une Keras application (deep learning pre-trained models\")\n",
        "        print(\"https://keras.io/api/applications/ \")\n",
        "        \n",
        "        image_shape = image_size + (3,) \n",
        "        model= get_model(image_shape)\n",
        "        \n",
        "        print(\"\\ntraining\")\n",
        "        # training du modèle et sauvegarde du meilleur modèle (max val_accuracy) rencontré lors des epochs (sauvegarde des poids)\n",
        "        Network = train(model, train_dataset, valid_dataset,epochs=15, len_train=len_train, batch_size=16, filenum=n)\n",
        "         \n",
        "                 \n",
        "        \n",
        "        # faire une prediction du validation set à partir du modèle et la comparer aux vrais labels\n",
        "        # ça équivaut à un accuracy_score de sklearn.\n",
        "        # attention : c'est pas équivalent au model.evaluate(valid_dataset) de keras, ça ne donne pas les mêmes résultats car \n",
        "        # il s'agit d'une classification Multi-Class et multi-label.\n",
        "        \n",
        "        print(\"\\nvalidation\")\n",
        "        val_accuracy = validate(model, valid_dataset,len_valid,n)\n",
        "        \n",
        "        \n",
        "        # courbes de loss / validation loss et courbe d'accuracy / validation accuracy\n",
        "        loss_accuracy_curves(Network)   \n",
        "        \n",
        "        # Sauvegarde de la val_accuracy dans uen liste pour un calcul de moyenne et d'écart-type\n",
        "        print('>>>>>>>>>>>>>>>>>>>>>>>>>>> validation_accuracy %.3f' % val_accuracy)\n",
        "        list_val_accuracy.append(val_accuracy)\n",
        "        \n",
        "        print(\"filenum= \",n)\n",
        "        n +=1 \n",
        "        \n",
        "        \n",
        "        break;\n",
        "    return list_val_accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "list_val_accuracy= evaluate_model_plant_patho(X,y)\n",
        "duree_totale = time()- debut\n",
        "print(\"durée du training avec 3-fold cross-validation  (en secondes) =\", duree_totale)\n",
        "print('Validation Accuracy: moyenne %.3f ecart-type(%.3f)' % (mean(list_val_accuracy), std(list_val_accuracy)))   \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split des données entre train set et validation set en utlisant la 3-fold cross validation \n",
            "(12421,) (12421, 5)\n",
            "(6211,) (6211, 5)\n",
            "12421 6211\n",
            "\n",
            "Création des datasets train, validation et test\n",
            "\n",
            "Création du modèle à partir d'une Keras application (deep learning pre-trained models\n",
            "https://keras.io/api/applications/ \n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "inception_resnet_v2 (Functio (None, 1536)              54336736  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 7685      \n",
            "=================================================================\n",
            "Total params: 54,344,421\n",
            "Trainable params: 54,283,877\n",
            "Non-trainable params: 60,544\n",
            "_________________________________________________________________\n",
            "\n",
            "training\n",
            "Epoch 1/15\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "776/776 [==============================] - 5539s 7s/step - loss: 0.4801 - accuracy: 0.4341 - val_loss: 3787.5349 - val_accuracy: 0.5634\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.56336, saving model to drive/MyDrive/Colab Notebooks/Plant/IncRestV2-best-dataset-062021-1.hdf5\n",
            "Epoch 2/15\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 1.8000000000000004e-05.\n",
            "776/776 [==============================] - 4011s 5s/step - loss: 0.3479 - accuracy: 0.5693 - val_loss: 9133.2090 - val_accuracy: 0.5820\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.56336 to 0.58203, saving model to drive/MyDrive/Colab Notebooks/Plant/IncRestV2-best-dataset-062021-1.hdf5\n",
            "Epoch 3/15\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 2.6000000000000002e-05.\n",
            "776/776 [==============================] - 3924s 5s/step - loss: 0.3047 - accuracy: 0.6165 - val_loss: 1441.5895 - val_accuracy: 0.6039\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.58203 to 0.60393, saving model to drive/MyDrive/Colab Notebooks/Plant/IncRestV2-best-dataset-062021-1.hdf5\n",
            "Epoch 4/15\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 3.4000000000000007e-05.\n",
            "776/776 [==============================] - 3928s 5s/step - loss: 0.2788 - accuracy: 0.6451 - val_loss: 2647.9207 - val_accuracy: 0.5899\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.60393\n",
            "Epoch 5/15\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 4.2000000000000004e-05.\n",
            "776/776 [==============================] - 3923s 5s/step - loss: 0.2501 - accuracy: 0.6729 - val_loss: 6424.4746 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.60393 to 0.66672, saving model to drive/MyDrive/Colab Notebooks/Plant/IncRestV2-best-dataset-062021-1.hdf5\n",
            "Epoch 6/15\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 5e-05.\n",
            "652/776 [========================>.....] - ETA: 9:58 - loss: 0.2324 - accuracy: 0.7033 "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}